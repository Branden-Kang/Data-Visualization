{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMaA4T256hvtqdLFY44BTGd"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://towardsdatascience.com/aggregating-real-time-sensor-data-with-python-and-redpanda-30a139d59702)"
      ],
      "metadata": {
        "id": "nH4r9iiKrX5f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "import json\n",
        "import logging\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "from quixstreams import Application\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "TOPIC = \"raw-temp-data\" # defines the input topic\n",
        "SINK = \"agg-temperature\"  # defines the output topic\n",
        "WINDOW = 10  # defines the length of the time window in seconds\n",
        "WINDOW_EXPIRES = 1 # defines, in seconds, how late data can arrive before it is excluded from the window\n",
        "\n",
        "app = Application.Quix(\n",
        "       consumer_group=\"quix-stream-processor\",\n",
        "       auto_offset_reset=\"earliest\")\n",
        "\n",
        "input_topic = app.topic(TOPIC, value_deserializer=\"json\")\n",
        "output_topic = app.topic(SINK, value_serializer=\"json\")\n",
        "\n",
        "sdf = app.dataframe(input_topic)\n",
        "sdf = sdf.update(lambda value: logger.info(f\"Input value received: {value}\"))\n",
        "\n",
        "def custom_ts_extractor(value):\n",
        "\n",
        "    # Extract the sensor's timestamp and convert to a datetime object\n",
        "    dt_obj = datetime.strptime(value[\"ts\"], \"%Y-%m-%dT%H:%M:%S.%f\") #\n",
        "\n",
        "    # Convert to milliseconds since the Unix epoch for efficent procesing with Quix\n",
        "    milliseconds = int(dt_obj.timestamp() * 1000)\n",
        "    value[\"timestamp\"] = milliseconds\n",
        "    logger.info(f\"Value of new timestamp is: {value['timestamp']}\")\n",
        "\n",
        "    return value[\"timestamp\"]\n",
        "\n",
        "# Override the previously defined input_topic variable so that it uses the custom timestamp extractor\n",
        "input_topic = app.topic(TOPIC, timestamp_extractor=custom_ts_extractor, value_deserializer=\"json\")\n",
        "\n",
        "def initializer(value: dict) -> dict:\n",
        "\n",
        "    value_dict = json.loads(value)\n",
        "    return {\n",
        "        'count': 1,\n",
        "        'min': value_dict['value'],\n",
        "        'max': value_dict['value'],\n",
        "        'mean': value_dict['value'],\n",
        "    }\n",
        "\n",
        "def reducer(aggregated: dict, value: dict) -> dict:\n",
        "    aggcount = aggregated['count'] + 1\n",
        "    value_dict = json.loads(value)\n",
        "    return {\n",
        "        'count': aggcount,\n",
        "        'min': min(aggregated['min'], value_dict['value']),\n",
        "        'max': max(aggregated['max'], value_dict['value']),\n",
        "        'mean': (aggregated['mean'] * aggregated['count'] + value_dict['value']) / (aggregated['count'] + 1)\n",
        "    }\n",
        "\n",
        "### Define the window parameters such as type and length\n",
        "\n",
        "sdf = (\n",
        "    # Define a tumbling window of 10 seconds\n",
        "    sdf.tumbling_window(timedelta(seconds=WINDOW), grace_ms=timedelta(seconds=WINDOW_EXPIRES))\n",
        "\n",
        "    # Create a \"reduce\" aggregation with \"reducer\" and \"initializer\" functions\n",
        "    .reduce(reducer=reducer, initializer=initializer)\n",
        "\n",
        "    # Emit results only for closed 10 second windows\n",
        "    .final()\n",
        ")\n",
        "\n",
        "### Apply the window to the Streaming DataFrame and define the data points to include in the output\n",
        "sdf = sdf.apply(\n",
        "    lambda value: {\n",
        "        \"time\": value[\"end\"], # Use the window end time as the timestamp for message sent to the 'agg-temperature' topic\n",
        "        \"temperature\": value[\"value\"], # Send a dictionary of {count, min, max, mean} values for the temperature parameter\n",
        "    }\n",
        ")\n",
        "\n",
        "sdf = sdf.to_topic(output_topic)\n",
        "sdf = sdf.update(lambda value: logger.info(f\"Produced value: {value}\"))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logger.info(\"Starting application\")\n",
        "    app.run(sdf)\n",
        "```"
      ],
      "metadata": {
        "id": "MQEKVaHfrn1R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "FROM python:3.11.1-slim-buster\n",
        "\n",
        "ENV DEBIAN_FRONTEND=\"noninteractive\"\n",
        "ENV PYTHONUNBUFFERED=1\n",
        "ENV PYTHONIOENCODING=UTF-8\n",
        "\n",
        "WORKDIR /app\n",
        "COPY . .\n",
        "RUN find | grep requirements.txt | xargs -I '{}' python3 -m pip install -r '{}' --extra-index-url https://pkgs.dev.azure.com/quix-analytics/53f7fe95-59fe-4307-b479-2473b96de6d1/_packaging/public/pypi/simple/\n",
        "ENTRYPOINT [\"python3\", \"main.py\"]\n",
        "```"
      ],
      "metadata": {
        "id": "F3QTCLgpr8-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stream Processor\n",
        "import json\n",
        "import logging\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "from quixstreams import Application\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "TOPIC = \"raw-temp-data\" # defines the input topic\n",
        "SINK = \"agg-temperature\"  # defines the output topic\n",
        "WINDOW = 10  # defines the length of the time window in seconds\n",
        "WINDOW_EXPIRES = 1 # defines, in seconds, how late data can arrive before it is excluded from the window\n",
        "\n",
        "app = Application.Quix(\n",
        "       consumer_group=\"quix-stream-processor\",\n",
        "       auto_offset_reset=\"earliest\")\n",
        "\n",
        "input_topic = app.topic(TOPIC, value_deserializer=\"json\")\n",
        "output_topic = app.topic(SINK, value_serializer=\"json\")\n",
        "\n",
        "sdf = app.dataframe(input_topic)\n",
        "sdf = sdf.update(lambda value: logger.info(f\"Input value received: {value}\"))\n",
        "\n",
        "def custom_ts_extractor(value):\n",
        "\n",
        "    # Extract the sensor's timestamp and convert to a datetime object\n",
        "    dt_obj = datetime.strptime(value[\"ts\"], \"%Y-%m-%dT%H:%M:%S.%f\") #\n",
        "\n",
        "    # Convert to milliseconds since the Unix epoch for efficent procesing with Quix\n",
        "    milliseconds = int(dt_obj.timestamp() * 1000)\n",
        "    value[\"timestamp\"] = milliseconds\n",
        "    logger.info(f\"Value of new timestamp is: {value['timestamp']}\")\n",
        "\n",
        "    return value[\"timestamp\"]\n",
        "\n",
        "# Override the previously defined input_topic variable so that it uses the custom timestamp extractor\n",
        "input_topic = app.topic(TOPIC, timestamp_extractor=custom_ts_extractor, value_deserializer=\"json\")\n",
        "\n",
        "def initializer(value: dict) -> dict:\n",
        "\n",
        "    value_dict = json.loads(value)\n",
        "    return {\n",
        "        'count': 1,\n",
        "        'min': value_dict['value'],\n",
        "        'max': value_dict['value'],\n",
        "        'mean': value_dict['value'],\n",
        "    }\n",
        "\n",
        "def reducer(aggregated: dict, value: dict) -> dict:\n",
        "    aggcount = aggregated['count'] + 1\n",
        "    value_dict = json.loads(value)\n",
        "    return {\n",
        "        'count': aggcount,\n",
        "        'min': min(aggregated['min'], value_dict['value']),\n",
        "        'max': max(aggregated['max'], value_dict['value']),\n",
        "        'mean': (aggregated['mean'] * aggregated['count'] + value_dict['value']) / (aggregated['count'] + 1)\n",
        "    }\n",
        "\n",
        "### Define the window parameters such as type and length\n",
        "\n",
        "sdf = (\n",
        "    # Define a tumbling window of 10 seconds\n",
        "    sdf.tumbling_window(timedelta(seconds=WINDOW), grace_ms=timedelta(seconds=WINDOW_EXPIRES))\n",
        "\n",
        "    # Create a \"reduce\" aggregation with \"reducer\" and \"initializer\" functions\n",
        "    .reduce(reducer=reducer, initializer=initializer)\n",
        "\n",
        "    # Emit results only for closed 10 second windows\n",
        "    .final()\n",
        ")\n",
        "\n",
        "### Apply the window to the Streaming DataFrame and define the data points to include in the output\n",
        "sdf = sdf.apply(\n",
        "    lambda value: {\n",
        "        \"time\": value[\"end\"], # Use the window end time as the timestamp for message sent to the 'agg-temperature' topic\n",
        "        \"temperature\": value[\"value\"], # Send a dictionary of {count, min, max, mean} values for the temperature parameter\n",
        "    }\n",
        ")\n",
        "\n",
        "sdf = sdf.to_topic(output_topic)\n",
        "sdf = sdf.update(lambda value: logger.info(f\"Produced value: {value}\"))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logger.info(\"Starting application\")\n",
        "    app.run(sdf)"
      ],
      "metadata": {
        "id": "Cm9PbcOSrejP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "name: Stream Producer\n",
        "language: python\n",
        "variables:\n",
        "  - name: output\n",
        "    inputType: OutputTopic\n",
        "    description: The topic that stores the raw temperature data.\n",
        "    defaultValue: raw-temp-data\n",
        "    required: true\n",
        "dockerfile: dockerfile\n",
        "runEntryPoint: main.py\n",
        "defaultFile: main.py\n",
        "```"
      ],
      "metadata": {
        "id": "8nRRWnDbsEci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "FROM python:3.11.1-slim-buster\n",
        "\n",
        "ENV DEBIAN_FRONTEND=\"noninteractive\"\n",
        "ENV PYTHONUNBUFFERED=1\n",
        "ENV PYTHONIOENCODING=UTF-8\n",
        "\n",
        "WORKDIR /app\n",
        "COPY . .\n",
        "RUN find | grep requirements.txt | xargs -I '{}' python3 -m pip install -r '{}' --extra-index-url https://pkgs.dev.azure.com/quix-analytics/53f7fe95-59fe-4307-b479-2473b96de6d1/_packaging/public/pypi/simple/\n",
        "ENTRYPOINT [\"python3\", \"main.py\"]\n",
        "```"
      ],
      "metadata": {
        "id": "id0LBwMfsHXB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xGgDEWNVrWRA"
      },
      "outputs": [],
      "source": [
        "# Stream Producer\n",
        "from dataclasses import dataclass, asdict # used to define the data schema\n",
        "from datetime import datetime # used to manage timestamps\n",
        "from time import sleep  # used to slow down the data generator\n",
        "import uuid # used for message id creation\n",
        "import json # used for serializing data\n",
        "import random\n",
        "\n",
        "from quixstreams import Application\n",
        "\n",
        "app = Application.Quix()\n",
        "destination_topic = app.topic(name='raw-temp-data', value_serializer=\"json\")\n",
        "\n",
        "@dataclass\n",
        "class Temperature:\n",
        "    ts: datetime\n",
        "    value: int\n",
        "\n",
        "    def to_json(self):\n",
        "        # Convert the dataclass to a dictionary\n",
        "        data = asdict(self)\n",
        "        # Format the datetime object as a string\n",
        "        data['ts'] = self.ts.isoformat()\n",
        "        # Serialize the dictionary to a JSON string\n",
        "        return json.dumps(data)\n",
        "\n",
        "i = 0\n",
        "with app.get_producer() as producer:\n",
        "    while i < 10000:\n",
        "        sensor_id = random.choice([\"Sensor1\", \"Sensor2\", \"Sensor3\", \"Sensor4\", \"Sensor5\"])\n",
        "        temperature = Temperature(datetime.now(), random.randint(0, 100))\n",
        "        value = temperature.to_json()\n",
        "\n",
        "        print(f\"Producing value {value}\")\n",
        "        serialized = destination_topic.serialize(\n",
        "            key=sensor_id, value=value, headers={\"uuid\": str(uuid.uuid4())}\n",
        "        )\n",
        "        producer.produce(\n",
        "            topic=destination_topic.name,\n",
        "            headers=serialized.headers,\n",
        "            key=serialized.key,\n",
        "            value=serialized.value,\n",
        "        )\n",
        "        i += 1\n",
        "        sleep(random.randint(0, 1000) / 1000)"
      ]
    }
  ]
}